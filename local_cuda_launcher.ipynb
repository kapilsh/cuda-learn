{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ab5defd-fd0a-4dbd-9410-c160e20c545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_code_file = \"./src/gpu.cu\"\n",
    "header_code_file = \"./src/gpu.hpp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c6d96b-935a-4793-a57c-b27811b1e75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "void printCudaVersion()\n",
      "{\n",
      "    std::cout << \"CUDA Compiled version: \" << __CUDACC_VER_MAJOR__ << \".\" << __CUDACC_VER_MINOR__ << std::endl;\n",
      "\n",
      "    int runtime_ver;\n",
      "    cudaRuntimeGetVersion(&runtime_ver);\n",
      "    std::cout << \"CUDA Runtime version: \" << runtime_ver << std::endl;\n",
      "\n",
      "    int driver_ver;\n",
      "    cudaDriverGetVersion(&driver_ver);\n",
      "    std::cout << \"CUDA Driver version: \" << driver_ver << std::endl;\n",
      "}\n",
      "\n",
      "__global__\n",
      "void saxpy(int n, float a, float *x, float *y) {\n",
      "    int i = blockIdx.x*blockDim.x + threadIdx.x;\n",
      "    if (i < n) {\n",
      "        y[i] = a*x[i] + y[i];\n",
      "    }\n",
      "}\n",
      "\n",
      "torch::Tensor saxpy_wrapper(const torch::Tensor& x, torch::Tensor y, float a) {\n",
      "    auto n = static_cast<int32_t>(torch::numel(x));\n",
      "    saxpy<<<n, 1>>>(n, a, x.data_ptr<float>(), y.data_ptr<float>());\n",
      "    std::cout <<  \"Calculated saxpy\\n\";\n",
      "    cudaDeviceSynchronize();\n",
      "    return y;\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(cuda_code_file) as f:\n",
    "    cuda_code = \"\".join([f for f in f.readlines() if not f.startswith(\"#include\")])\n",
    "    print(cuda_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c445d909-0802-4df7-ac4a-0c429edb956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "void printCudaVersion();\n",
      "\n",
      "torch::Tensor saxpy_wrapper(const torch::Tensor& x, torch::Tensor y, float a);\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(header_code_file) as f:\n",
    "    header_code = \"\".join([f for f in f.readlines() if not f.startswith(\"#include\")])\n",
    "    print(header_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b97d7f4f-37e6-4e15-a443-408841fad5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ./build/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f00df398-4b04-4155-bdb6-21eed6012d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file ./build/build.ninja...\n",
      "Building extension module saxpy_extension...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=saxpy_extension -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/ksharma/anaconda3/envs/cuda-learn/lib/python3.12/site-packages/torch/include -isystem /home/ksharma/anaconda3/envs/cuda-learn/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /home/ksharma/anaconda3/envs/cuda-learn/lib/python3.12/site-packages/torch/include/TH -isystem /home/ksharma/anaconda3/envs/cuda-learn/lib/python3.12/site-packages/torch/include/THC -isystem /home/ksharma/anaconda3/envs/cuda-learn/include -isystem /home/ksharma/anaconda3/envs/cuda-learn/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/ksharma/dev/git/cuda-learn/build/main.cpp -o main.o \n",
      "[2/3] /home/ksharma/anaconda3/envs/cuda-learn/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=saxpy_extension -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/ksharma/anaconda3/envs/cuda-learn/lib/python3.12/site-packages/torch/include -isystem /home/ksharma/anaconda3/envs/cuda-learn/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /home/ksharma/anaconda3/envs/cuda-learn/lib/python3.12/site-packages/torch/include/TH -isystem /home/ksharma/anaconda3/envs/cuda-learn/lib/python3.12/site-packages/torch/include/THC -isystem /home/ksharma/anaconda3/envs/cuda-learn/include -isystem /home/ksharma/anaconda3/envs/cuda-learn/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -O2 -std=c++17 -c /home/ksharma/dev/git/cuda-learn/build/cuda.cu -o cuda.cuda.o \n",
      "[3/3] c++ main.o cuda.cuda.o -shared -L/home/ksharma/anaconda3/envs/cuda-learn/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/home/ksharma/anaconda3/envs/cuda-learn/lib -lcudart -o saxpy_extension.so\n",
      "CUDA Compiled version: 12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module saxpy_extension...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Runtime version: 11070\n",
      "CUDA Driver version: 12020\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "saxpy_extension = load_inline(\n",
    "    name='saxpy_extension',\n",
    "    cpp_sources=header_code,\n",
    "    cuda_sources=cuda_code,\n",
    "    functions=['saxpy_wrapper', \"printCudaVersion\"],\n",
    "    with_cuda=True,\n",
    "    verbose=True,\n",
    "    extra_cuda_cflags=[\"-O2\"],\n",
    "    build_directory='./build',\n",
    "    # extra_cuda_cflags=['--expt-relaxed-constexpr']\n",
    ")\n",
    "\n",
    "a = torch.tensor([[1., 2., 3.], [4., 5., 6.]], device='cuda')\n",
    "saxpy_extension.printCudaVersion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e8c4ee7-c140-48e7-9d01-3105c9b7a8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1994,  1.0400,  1.0288],\n",
      "        [ 0.4695, -0.1891,  0.3379]], device='cuda:0')\n",
      "tensor([[-1.5099,  0.4243, -1.9025],\n",
      "        [ 0.7954, -0.9438,  0.2487]], device='cuda:0')\n",
      "Calculated saxpy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1112,  2.5043,  0.1552],\n",
       "        [ 1.7344, -1.3219,  0.9246]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((2, 3), device=\"cuda\")\n",
    "y = torch.randn((2, 3), device=\"cuda\")\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "saxpy_extension.saxpy_wrapper(x, y, 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a0b3f-b2d6-4d30-a7b4-53598d6ef3a4",
   "metadata": {},
   "source": [
    "# Pytorch Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e89598d-a0a8-4c81-9e8c-0b40ff0cb257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "         cudaLaunchKernel        99.31%       4.149ms        99.31%       4.149ms       4.149ms             1  \n",
      "    cudaDeviceSynchronize         0.69%      29.000us         0.69%      29.000us      14.500us             2  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 4.178ms\n",
      "\n",
      "Calculated saxpy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-03-28 07:16:35 431270:431270 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-03-28 07:16:35 431270:431270 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-03-28 07:16:35 431270:431270 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    saxpy_extension.saxpy_wrapper(x, y, 2.0)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c58c8a8-e278-4902-8f2b-653b9617beb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated saxpy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-03-28 07:16:50 431270:431270 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated saxpy\n",
      "Calculated saxpy\n",
      "Calculated saxpy\n",
      "-------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "    saxpy(int, float, float*, float*)         0.00%       0.000us         0.00%       0.000us       0.000us     230.632ms        57.62%     230.632ms     115.316ms             2  \n",
      "                          aten::copy_         0.00%      75.000us         6.07%     170.139ms      42.535ms     169.616ms        42.38%     169.616ms      42.404ms             4  \n",
      "     Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     169.616ms        42.38%     169.616ms      42.404ms             4  \n",
      "                        ProfilerStep*         2.85%      79.893ms       100.00%        2.803s        1.402s       0.000us         0.00%     169.616ms      84.808ms             2  \n",
      "                          aten::randn         0.00%      66.000us        82.84%        2.322s     580.586ms       0.000us         0.00%       0.000us       0.000us             4  \n",
      "                          aten::empty         0.00%      91.000us         0.00%      91.000us      22.750us       0.000us         0.00%       0.000us       0.000us             4  \n",
      "                        aten::normal_        82.84%        2.322s        82.84%        2.322s     580.547ms       0.000us         0.00%       0.000us       0.000us             4  \n",
      "                             aten::to         0.00%      58.000us         6.08%     170.302ms      42.575ms       0.000us         0.00%     169.616ms      42.404ms             4  \n",
      "                       aten::_to_copy         0.00%      44.000us         6.07%     170.244ms      42.561ms       0.000us         0.00%     169.616ms      42.404ms             4  \n",
      "                  aten::empty_strided         0.00%      61.000us         0.00%      61.000us      15.250us       0.000us         0.00%       0.000us       0.000us             4  \n",
      "                      cudaMemcpyAsync         6.05%     169.681ms         6.05%     169.681ms      42.420ms       0.000us         0.00%       0.000us       0.000us             4  \n",
      "                cudaStreamSynchronize         0.01%     383.000us         0.01%     383.000us      95.750us       0.000us         0.00%       0.000us       0.000us             4  \n",
      "                     cudaLaunchKernel         8.23%     230.697ms         8.23%     230.697ms     115.349ms       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                cudaDeviceSynchronize         0.00%      19.000us         0.00%      19.000us       6.333us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "-------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.803s\n",
      "Self CUDA time total: 400.248ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-03-28 07:16:53 431270:431270 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-03-28 07:16:53 431270:431270 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated saxpy\n",
      "Calculated saxpy\n",
      "Calculated saxpy\n",
      "Calculated saxpy\n",
      "Calculated saxpy\n",
      "Calculated saxpy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import time\n",
    "\n",
    "\n",
    "# ## Default way to use profiler\n",
    "# with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "#     for _ in range(10):\n",
    "#         a = torch.square(torch.randn(10000, 10000).cuda())\n",
    "\n",
    "# prof.export_chrome_trace(\"trace.json\")\n",
    "\n",
    "\n",
    "## With warmup and skip\n",
    "# https://pytorch.org/docs/stable/profiler.html\n",
    "\n",
    "# Non-default profiler schedule allows user to turn profiler on and off\n",
    "# on different iterations of the training loop;\n",
    "# trace_handler is called every time a new trace becomes available\n",
    "def trace_handler(prof):\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_time_total\", row_limit=-1))\n",
    "    prof.export_chrome_trace(f\"/tmp/{int(time.time())}_test_trace_\" + str(prof.step_num) + \".json\")\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "\n",
    "    # In this example with wait=1, warmup=1, active=2, repeat=1,\n",
    "    # profiler will skip the first step/iteration,\n",
    "    # start warming up on the second, record\n",
    "    # the third and the forth iterations,\n",
    "    # after which the trace will become available\n",
    "    # and on_trace_ready (when set) is called;\n",
    "    # the cycle repeats starting with the next step\n",
    "\n",
    "    schedule=torch.profiler.schedule(\n",
    "        wait=1,\n",
    "        warmup=1,\n",
    "        active=2,\n",
    "        repeat=1),\n",
    "    on_trace_ready=trace_handler\n",
    "    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')\n",
    "    # used when outputting for tensorboard\n",
    "    ) as p:\n",
    "        for iter in range(10):\n",
    "            saxpy_extension.saxpy_wrapper(torch.randn(10000, 10000).cuda(), torch.randn(10000, 10000).cuda(), 2.0)\n",
    "            # send a signal to the profiler that the next iteration has started\n",
    "            p.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f66e148-9be5-4b7b-9cf3-77c4dd126c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
